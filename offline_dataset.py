"""
python offline_dataset.py \
  --config_file lora.toml \
  --lora_path /root/autodl-tmp/output/NewBieLoRA \
  --output_dir /root/autodl-tmp/gen_dataset \
  --num_samples 3 \
  --steps 28
"""

import os
import json
import toml
import torch
import argparse
import math
from pathlib import Path
from tqdm import tqdm
from PIL import Image

# å¯¼å…¥ä»“åº“ç»„ä»¶
from dataset import ImageCaptionDataset, collate_fn
from train_newbie_lora import load_model_and_tokenizer, setup_lora
from peft import set_peft_model_state_dict
from safetensors.torch import load_file
from transport import create_transport
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler

def get_lin_function(x1: float = 256, y1: float = 0.5, x2: float = 4096, y2: float = 1.15):
    m = (y2 - y1) / (x2 - x1)
    b = y1 - m * x1
    return lambda x: m * x + b

def parse_args():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ DPO ç¦»çº¿åå¥½æ•°æ®é›†æ ·æœ¬æ± ")
    parser.add_argument("--config_file", type=str, required=True, help="è®­ç»ƒæ—¶ä½¿ç”¨çš„ .toml é…ç½®æ–‡ä»¶ (lora.toml)")
    parser.add_argument("--lora_path", type=str, required=True, help="SFT è®­ç»ƒäº§å‡ºçš„ LoRA æƒé‡è·¯å¾„ (PEFT æ–‡ä»¶å¤¹ç›®å½•)")
    parser.add_argument("--output_dir", type=str, default="./sdpo_data_v1", help="æ ·æœ¬å›¾ç‰‡å­˜æ”¾ç›®å½•")
    parser.add_argument("--num_samples", type=int, default=3, help="æ¯ä¸ª Prompt ç”Ÿæˆçš„æ ·æœ¬æ•°é‡ (N)")
    parser.add_argument("--steps", type=int, default=28, help="ç”Ÿæˆæ­¥æ•°")
    parser.add_argument("--cfg_scale", type=float, default=6, help="Classifier-Free Guidance å¼ºåº¦")
    parser.add_argument("--device", type=str, default="cuda", help="ä½¿ç”¨è®¾å¤‡")
    return parser.parse_args()

@torch.inference_mode()
def main():
    args = parse_args()
    
    # 1. åŠ è½½é…ç½®
    with open(args.config_file, 'r', encoding='utf-8') as f:
        config = toml.load(f)
    
    # å…³é—­æ•°æ®å¢å¼ºéƒ¨åˆ†ä»¥ä¿è¯æ•°æ®å¯¹åº”å…³ç³»
    config['Model']['shuffle_caption'] = False
    config['Model']['caption_dropout_rate'] = 0.0
    config['Model']['caption_tag_dropout_rate'] = 0.0
    config['Model']['drop_artist_rate'] = 0.0
    
    os.makedirs(args.output_dir, exist_ok=True)
    gen_images_dir = os.path.join(args.output_dir, "generated")
    os.makedirs(gen_images_dir, exist_ok=True)

    # 2. åŠ è½½æ¨¡å‹ä¸ LoRA
    # æ³¨æ„ï¼šç”Ÿæˆéœ€è¦å®Œæ•´çš„ VAE å’Œ Text Encoder
    model, vae, text_encoder, tokenizer, clip_model, clip_tokenizer = load_model_and_tokenizer(config)
    
    # åº”ç”¨ LoRA
    if os.path.isdir(args.lora_path):
        # å¦‚æœæ˜¯ PEFT ç›®å½•ï¼Œç›´æ¥ä» Base Model åŠ è½½
        from peft import PeftModel
        print(f"Loading LoRA from: {args.lora_path}")
        model = PeftModel.from_pretrained(model, args.lora_path)
    else:
        # åªæœ‰åœ¨è·¯å¾„æ— æ•ˆï¼ˆæˆ–æƒ³æµ‹è¯•æœªè®­ç»ƒçš„éšæœºåˆå§‹åŒ– LoRAï¼‰æ—¶ï¼Œæ‰æ‰‹åŠ¨ setup_lora
        print(f'{args.lora_path} is not a valid PEFT LoRA directory path! Falling back to random init.')
        model = setup_lora(model, config)
    
    model.to(args.device).eval()
    vae.to(args.device).eval()
    text_encoder.to(args.device).eval()
    clip_model.to(args.device).eval()

    print("Compiling model...")
    model = torch.compile(model, mode="default", fullgraph=False, dynamic=True)

    # 3. åˆå§‹åŒ–æ•°æ®é›† (è·å–åŠ¨æ€åˆ†ç®±ç»“æœ)
    # ä¸å¯ç”¨ cache ä»¥ä¾¿ç›´æ¥è¯»å–åŸå§‹è·¯å¾„ä¿¡æ¯
    dataset = ImageCaptionDataset(
        train_data_dir=config['Model']['train_data_dir'],
        resolution=config['Model']['resolution'],
        enable_bucket=config['Model'].get('enable_bucket', True),
        use_cache=False,
        min_bucket_reso=config['Model'].get('min_bucket_reso', 256),
        max_bucket_reso=config['Model'].get('max_bucket_reso', 2048),
        bucket_reso_step=config['Model'].get('bucket_reso_step', 64),
        enable_wildcard=False, # æ­¤å¤„å…ˆå…³é—­ï¼Œæˆ‘ä»¬åœ¨å¾ªç¯ä¸­æ‰‹åŠ¨å¤„ç† split
    )

    # 4. é¢„è®¡ç®—æ— æ¡ä»¶ç‰¹å¾ (ç”¨äº CFG)
    print("é¢„è®¡ç®—è´Ÿå‘ (Unconditional) ç‰¹å¾...")
    # Gemma æ— æ¡ä»¶
    uncond_input = tokenizer(
        [""], padding=True, pad_to_multiple_of=8,
        truncation=True, max_length=1280, return_tensors="pt"
    ).to(args.device)
    uncond_outputs = text_encoder(**uncond_input, output_hidden_states=True)
    uncond_cap_feats = uncond_outputs.hidden_states[-2].to(dtype=torch.bfloat16)
    uncond_cap_mask = uncond_input.attention_mask
    # CLIP æ— æ¡ä»¶
    uncond_clip_input = clip_tokenizer(
        [""], padding=True, truncation=True,
        max_length=2048, return_tensors="pt"
    ).to(args.device)
    uncond_clip_text_pooled = clip_model.get_text_features(**uncond_clip_input).to(dtype=torch.bfloat16)

    # 5. å¼€å§‹ç”Ÿæˆå¾ªç¯
    results = []
    gemma3_prompt = config['Model'].get('gemma3_prompt', "")
    scaling_factor = getattr(vae.config, 'scaling_factor', 0.3611)
    shift_factor = getattr(vae.config, 'shift_factor', 0.1159)

    print(f"å¼€å§‹ä¸º {len(dataset)} ä¸ªåŸå§‹æ ·æœ¬ç”Ÿæˆåå¥½å¯¹æ± ...")
    
    for i in tqdm(range(len(dataset))):
        img_path = dataset.image_paths[i]
        raw_caption = dataset.captions[i]
        # è·å–è¯¥å›¾ç‰‡åœ¨åˆ†ç®±ç­–ç•¥ä¸‹çš„ç›®æ ‡åˆ†è¾¨ç‡
        target_width, target_height = dataset.image_to_bucket[i]

        # è®¡ç®—å½“å‰åˆ†è¾¨ç‡ä¸‹çš„ Time Shift å‚æ•°
        current_seq_len = (target_height // 16) * (target_width // 16)
        mu = get_lin_function()(current_seq_len) 
        shift_val = math.exp(mu)
        
        # å¤„ç† Wildcard: è·å–æ‰€æœ‰å¯èƒ½çš„ caption å˜ä½“
        captions_to_process = [raw_caption]
        if "<split>" in raw_caption:
            captions_to_process = [c.strip() for c in raw_caption.split("<split>") if c.strip()]
        captions_to_process = [c.replace(" ||| ", ", ") for c in captions_to_process]

        for cap_idx, caption in enumerate(captions_to_process):
            gen_paths = []
            # å‡†å¤‡ç¼–ç ç‰¹å¾
            with torch.no_grad():
                # ç¼–ç æ­£å‘ç‰¹å¾
                gemma_text = gemma3_prompt + caption
                pos_input = tokenizer([gemma_text], padding=True, pad_to_multiple_of=8,
                    truncation=True, max_length=1280, return_tensors="pt"
                ).to(args.device)
                pos_outputs = text_encoder(**pos_input, output_hidden_states=True)
                pos_cap_feats = pos_outputs.hidden_states[-2].to(dtype=torch.bfloat16)
                pos_cap_mask = pos_input.attention_mask
            
                pos_clip_input = clip_tokenizer(
                    [caption], padding=True, truncation=True,
                    max_length=2048, return_tensors="pt"
                ).to(args.device)
                pos_clip_text_pooled = clip_model.get_text_features(**pos_clip_input).to(dtype=torch.bfloat16)

                for n in range(args.num_samples):
                    # 1. åˆå§‹åŒ– Scheduler (æ¯ä¸ªåˆ†è¾¨ç‡å¯èƒ½éœ€è¦ä¸åŒçš„ shiftï¼Œæ‰€ä»¥å»ºè®®åœ¨å¾ªç¯å†…æˆ–æ ¹æ® shift ç¼“å­˜)
                    # num_train_timesteps=1000 æ˜¯ Diffusers é»˜è®¤ï¼Œé…åˆ shift ä½¿ç”¨
                    scheduler = FlowMatchEulerDiscreteScheduler(
                        num_train_timesteps=1000, 
                        shift=shift_val
                    )
                    scheduler.set_timesteps(args.steps)
                    
                    # 2. åˆå§‹åŒ–å™ªå£°
                    latents = torch.randn(1, 16, target_height // 8, target_width // 8, device=args.device, dtype=torch.bfloat16)
                    
                    # 3. é‡‡æ ·å¾ªç¯
                    for t in scheduler.timesteps:
                        t_norm = t / 1000.0
                        t_model = 1.0 - t_norm
                        t_tensor = torch.tensor([t_model], device=args.device, dtype=torch.bfloat16)

                        # 1. è®¡ç®—æ— æ¡ä»¶è¾“å‡º (Uncond)
                        v_uncond = model(
                            latents, 
                            t_tensor, 
                            cap_feats=uncond_cap_feats, 
                            cap_mask=uncond_cap_mask, 
                            clip_text_pooled=uncond_clip_text_pooled
                        )
                        
                        # 2. è®¡ç®—æœ‰æ¡ä»¶è¾“å‡º (Cond)
                        v_cond = model(
                            latents, 
                            t_tensor, 
                            cap_feats=pos_cap_feats, 
                            cap_mask=pos_cap_mask, 
                            clip_text_pooled=pos_clip_text_pooled
                        )
                        
                        # æ‹†åˆ†é¢„æµ‹é€Ÿåº¦ v å¹¶è¿›è¡Œ CFG æ··åˆ
                        v_final = v_uncond + args.cfg_scale * (v_cond - v_uncond)
                        
                        # ä½¿ç”¨ Scheduler æ­¥è¿›æ›´æ–°å™ªå£°ï¼Œæ³¨æ„å–åä»¥é€‚é…ç§¯åˆ†æ–¹å‘
                        latents = scheduler.step(-v_final, t, latents).prev_sample
                
                    # VAE è§£ç 
                    latents = (latents / scaling_factor) + shift_factor
                    image = vae.decode(latents.to(vae.dtype)).sample
                    image = (image / 2 + 0.5).clamp(0, 1)
                    image = image.cpu().permute(0, 2, 3, 1).float().numpy()[0]
                    image = Image.fromarray((image * 255).astype("uint8"))
                
                    # ä¿å­˜å›¾ç‰‡
                    safe_name = Path(img_path).stem
                    gen_filename = f"{safe_name}_cap{cap_idx}_sample{n}.jpg"
                    gen_path = os.path.join(gen_images_dir, gen_filename)
                    image.save(gen_path, quality=100)
                    gen_paths.append(os.path.abspath(gen_path))

                # è®°å½•æ•°æ®ç»“æ„
                results.append({
                    "caption": caption,
                    "real_image_path": os.path.abspath(img_path),
                    "generated_image_paths": gen_paths,
                    "resolution": [target_width, target_height],
                    "meta": {
                        "lora_source": args.lora_path,
                        "steps": args.steps,
                        "cfg_scale": args.cfg_scale,
                        "sampler": "euler"
                    }
                })

    # 6. è¾“å‡º JSON æ•°æ®åº“
    output_json = os.path.join(args.output_dir, "dataset.json")
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ‰ ç¦»çº¿æ•°æ®åº“å·²å»ºç«‹: {output_json}")
    print(f"æ€»è®¡æ ·æœ¬æ•°: {len(results)}")

if __name__ == "__main__":
    main()
