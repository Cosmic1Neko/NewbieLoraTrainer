"""
python offline_dataset.py \
  --config_file lora.toml \
  --lora_path /root/autodl-tmp/output/NewBieLoRA \
  --output_dir /root/autodl-tmp/gen_dataset \
  --num_samples 2 \
  --steps 28
"""

import os
import json
import toml
import torch
import argparse
import math
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import numpy as np

# å¯¼å…¥ä»“åº“ç»„ä»¶
from dataset import ImageCaptionDataset, collate_fn
from train_newbie_lora import load_model_and_tokenizer, setup_lora
from peft import set_peft_model_state_dict
from safetensors.torch import load_file
from transport import create_transport
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler

def get_lin_function(x1: float = 256, y1: float = 0.5, x2: float = 4096, y2: float = 1.15):
    m = (y2 - y1) / (x2 - x1)
    b = y1 - m * x1
    return lambda x: m * x + b

def parse_args():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ DPO ç¦»çº¿åå¥½æ•°æ®é›†æ ·æœ¬æ± ")
    parser.add_argument("--config_file", type=str, required=True, help="è®­ç»ƒæ—¶ä½¿ç”¨çš„ .toml é…ç½®æ–‡ä»¶ (lora.toml)")
    parser.add_argument("--lora_path", type=str, required=True, help="SFT è®­ç»ƒäº§å‡ºçš„ LoRA æƒé‡è·¯å¾„ (PEFT æ–‡ä»¶å¤¹ç›®å½•)")
    parser.add_argument("--output_dir", type=str, default="./sdpo_data_v1", help="æ ·æœ¬å›¾ç‰‡å­˜æ”¾ç›®å½•")
    parser.add_argument("--num_samples", type=int, default=3, help="æ¯ä¸ª Prompt ç”Ÿæˆçš„æ ·æœ¬æ•°é‡ (N)")
    parser.add_argument("--steps", type=int, default=28, help="ç”Ÿæˆæ­¥æ•°")
    parser.add_argument("--cfg_scale", type=float, default=6, help="Classifier-Free Guidance å¼ºåº¦")
    parser.add_argument("--device", type=str, default="cuda", help="ä½¿ç”¨è®¾å¤‡")
    return parser.parse_args()

@torch.inference_mode()
def main():
    args = parse_args()
    
    # 1. åŠ è½½é…ç½®
    with open(args.config_file, 'r', encoding='utf-8') as f:
        config = toml.load(f)
    
    # å…³é—­æ•°æ®å¢å¼ºéƒ¨åˆ†ä»¥ä¿è¯æ•°æ®å¯¹åº”å…³ç³»
    config['Model']['shuffle_caption'] = False
    config['Model']['caption_dropout_rate'] = 0.0
    config['Model']['caption_tag_dropout_rate'] = 0.0
    config['Model']['drop_artist_rate'] = 0.0
    
    os.makedirs(args.output_dir, exist_ok=True)
    gen_images_dir = os.path.join(args.output_dir, "generated")
    os.makedirs(gen_images_dir, exist_ok=True)

    # 2. åŠ è½½æ¨¡å‹ä¸ LoRA
    # æ³¨æ„ï¼šç”Ÿæˆéœ€è¦å®Œæ•´çš„ VAE å’Œ Text Encoder
    model, vae, text_encoder, tokenizer, clip_model, clip_tokenizer = load_model_and_tokenizer(config)
    
    # åº”ç”¨ LoRA
    if os.path.isdir(args.lora_path):
        # å¦‚æœæ˜¯ PEFT ç›®å½•ï¼Œç›´æ¥ä» Base Model åŠ è½½
        from peft import PeftModel
        print(f"Loading LoRA from: {args.lora_path}")
        model = PeftModel.from_pretrained(model, args.lora_path)
    else:
        # åªæœ‰åœ¨è·¯å¾„æ— æ•ˆï¼ˆæˆ–æƒ³æµ‹è¯•æœªè®­ç»ƒçš„éšæœºåˆå§‹åŒ– LoRAï¼‰æ—¶ï¼Œæ‰æ‰‹åŠ¨ setup_lora
        print(f'{args.lora_path} is not a valid PEFT LoRA directory path! Falling back to random init.')
        model = setup_lora(model, config)
    
    model.to(args.device, dtype=torch.bfloat16).eval()
    vae.to(args.device, dtype=torch.bfloat16).eval()
    text_encoder.to(args.device, dtype=torch.bfloat16).eval()
    clip_model.to(args.device, dtype=torch.bfloat16).eval()

    print("Compiling model...")
    model = torch.compile(model, mode="default", fullgraph=False, dynamic=True) # "reduce-overhead"

    # 3. åˆå§‹åŒ–æ•°æ®é›† (è·å–åŠ¨æ€åˆ†ç®±ç»“æœ)
    # ä¸å¯ç”¨ cache ä»¥ä¾¿ç›´æ¥è¯»å–åŸå§‹è·¯å¾„ä¿¡æ¯
    dataset = ImageCaptionDataset(
        train_data_dir=config['Model']['train_data_dir'],
        resolution=config['Model']['resolution'],
        enable_bucket=config['Model'].get('enable_bucket', True),
        use_cache=False,
        min_bucket_reso=config['Model'].get('min_bucket_reso', 256),
        max_bucket_reso=config['Model'].get('max_bucket_reso', 2048),
        bucket_reso_step=config['Model'].get('bucket_reso_step', 64),
        enable_wildcard=False, # æ­¤å¤„å…ˆå…³é—­ï¼Œæˆ‘ä»¬åœ¨å¾ªç¯ä¸­æ‰‹åŠ¨å¤„ç† split
    )

    # 4. é¢„è®¡ç®—æ— æ¡ä»¶ç‰¹å¾ (ç”¨äº CFG)
    print("é¢„è®¡ç®—è´Ÿå‘ (Unconditional) ç‰¹å¾...")
    # Gemma æ— æ¡ä»¶
    uncond_input = tokenizer(
        [""], padding="max_length", pad_to_multiple_of=8,
        truncation=True, max_length=1280, return_tensors="pt"
    ).to(args.device)
    with torch.no_grad():
        uncond_outputs = text_encoder(**uncond_input, output_hidden_states=True)
        uncond_cap_feats = uncond_outputs.hidden_states[-2].to(dtype=torch.bfloat16)
        uncond_cap_mask = uncond_input.attention_mask
    # CLIP æ— æ¡ä»¶
    uncond_clip_input = clip_tokenizer(
        [""], padding=True, truncation=True,
        max_length=2048, return_tensors="pt"
    ).to(args.device)
    with torch.no_grad():
        uncond_clip_text_pooled = clip_model.get_text_features(**uncond_clip_input).to(dtype=torch.bfloat16)

    # 5. å¼€å§‹ç”Ÿæˆå¾ªç¯
    results = []
    gemma3_prompt = config['Model'].get('gemma3_prompt', "")
    scaling_factor = getattr(vae.config, 'scaling_factor', 0.3611)
    shift_factor = getattr(vae.config, 'shift_factor', 0.1159)
 
    print(f"å¼€å§‹ä¸º {len(dataset)} ä¸ªåŸå§‹æ ·æœ¬ç”Ÿæˆåå¥½å¯¹æ± ...")

    batch_size = 2 # args.num_samples
    
    for i in tqdm(range(len(dataset))):
        img_path = dataset.image_paths[i]
        raw_caption = dataset.captions[i]
        target_width, target_height = dataset.image_to_bucket[i]
        
        # å¤„ç† Wildcard: è·å–æ‰€æœ‰å¯èƒ½çš„ caption å˜ä½“
        captions_to_process = [raw_caption]
        if "<split>" in raw_caption:
            captions_to_process = [c.strip() for c in raw_caption.split("<split>") if c.strip()]
        captions_to_process = [c.replace(" ||| ", ", ") for c in captions_to_process]

        for cap_idx, caption in enumerate(captions_to_process):
            gen_paths = []
            # å‡†å¤‡ç¼–ç ç‰¹å¾
            with torch.no_grad():
                # ç¼–ç æ­£å‘ç‰¹å¾
                gemma_text = gemma3_prompt + caption
                pos_input = tokenizer([gemma_text], padding="longest", pad_to_multiple_of=8,
                    truncation=True, max_length=1280, return_tensors="pt"
                ).to(args.device)
                current_seq_len = pos_input.input_ids.shape[1]
                pos_outputs = text_encoder(**pos_input, output_hidden_states=True)
                pos_cap_feats = pos_outputs.hidden_states[-2].to(dtype=torch.bfloat16)
                pos_cap_mask = pos_input.attention_mask
            
                pos_clip_input = clip_tokenizer(
                    [caption], padding=True, truncation=True,
                    max_length=2048, return_tensors="pt"
                ).to(args.device)
                pos_clip_text_pooled = clip_model.get_text_features(**pos_clip_input).to(dtype=torch.bfloat16)

                # 1. é‡å¤ uncond ç‰¹å¾ N æ¬¡
                batch_uncond_feats = uncond_cap_feats.repeat(batch_size, 1, 1)[:, :current_seq_len, :]
                batch_uncond_mask = uncond_cap_mask.repeat(batch_size, 1)[:, :current_seq_len]
                batch_uncond_pooled = uncond_clip_text_pooled.repeat(batch_size, 1)

                # 2. é‡å¤ cond ç‰¹å¾ N æ¬¡
                batch_pos_feats = pos_cap_feats.repeat(batch_size, 1, 1)
                batch_pos_mask = pos_cap_mask.repeat(batch_size, 1)
                batch_pos_pooled = pos_clip_text_pooled.repeat(batch_size, 1)

                # 3. æ‹¼æ¥ (Concat) -> å½¢çŠ¶å˜æˆ [2*N, Seq, Dim]
                cat_cap_feats = torch.cat([batch_uncond_feats, batch_pos_feats])
                cat_cap_mask = torch.cat([batch_uncond_mask, batch_pos_mask])
                cat_clip_pooled = torch.cat([batch_uncond_pooled, batch_pos_pooled])

                ######################################################################
                # 1. åˆå§‹åŒ– Scheduler
                scheduler = FlowMatchEulerDiscreteScheduler(num_train_timesteps=1000, use_dynamic_shifting=True)
                mu = get_lin_function()((target_height // 16) * (target_width // 16))
                scheduler.set_timesteps(args.steps, mu=mu)
                    
                # 2. åˆå§‹åŒ–å™ªå£°
                latents = torch.randn(batch_size, 16, target_height // 8, target_width // 8, device=args.device, dtype=torch.bfloat16)
                    
                # 3. é‡‡æ ·å¾ªç¯
                for t in scheduler.timesteps:
                    t_norm = t / 1000.0
                    t_model = 1.0 - t_norm
                    t_tensor = torch.full((batch_size * 2,), t_model, device=args.device, dtype=torch.bfloat16)

                    # Forward
                    v_pred = model(
                        torch.cat([latents, latents]), 
                        t_tensor, 
                        cap_feats=cat_cap_feats, 
                        cap_mask=cat_cap_mask, 
                        clip_text_pooled=cat_clip_pooled
                    )
                        
                    # æ‹†åˆ†é¢„æµ‹é€Ÿåº¦ v å¹¶è¿›è¡Œ CFG æ··åˆ
                    v_uncond, v_cond = v_pred.chunk(2)
                    v_final = v_uncond + args.cfg_scale * (v_cond - v_uncond)
                        
                    # ä½¿ç”¨ Scheduler æ­¥è¿›æ›´æ–°å™ªå£°ï¼Œæ³¨æ„å–åä»¥é€‚é…ç§¯åˆ†æ–¹å‘
                    latents = scheduler.step(-v_final, t, latents).prev_sample
                
                # 4. VAE è§£ç 
                latents = (latents / scaling_factor) + shift_factor
                image = vae.decode(latents.to(vae.dtype)).sample
                image = (image / 2 + 0.5).clamp(0, 1)
                image = image.cpu().permute(0, 2, 3, 1).float().numpy()
                
                # ä¿å­˜å›¾ç‰‡
                gen_paths = []
                safe_name = Path(img_path).stem
                for n in range(batch_size):
                    img_pil = Image.fromarray((image[n] * 255).astype("uint8"))
                    gen_filename = f"{safe_name}_cap{cap_idx}_sample{n}.jpg"
                    gen_path = os.path.join(gen_images_dir, gen_filename)
                    img_pil.save(gen_path, quality=100)
                    gen_paths.append(os.path.abspath(gen_path))

                # è®°å½•æ•°æ®ç»“æ„
                results.append({
                    "caption": caption,
                    "real_image_path": os.path.abspath(img_path),
                    "generated_image_paths": gen_paths,
                    "resolution": [target_width, target_height],
                    "meta": {
                        "lora_source": args.lora_path,
                        "steps": args.steps,
                        "cfg_scale": args.cfg_scale,
                        "sampler": "euler"
                    }
                })

    # 6. è¾“å‡º JSON æ•°æ®åº“
    output_json = os.path.join(args.output_dir, "dataset.json")
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ‰ ç¦»çº¿æ•°æ®åº“å·²å»ºç«‹: {output_json}")
    print(f"æ€»è®¡æ ·æœ¬æ•°: {len(results)}")

if __name__ == "__main__":
    main()
