"""
python offline_dataset.py \
  --config_file lora.toml \
  --lora_path /root/autodl-tmp/output/NewBieLoRA_step_1000 \
  --output_dir ./sdpo_data_v1 \
  --num_samples 4 \
  --steps 28
"""

import os
import json
import toml
import torch
import argparse
from pathlib import Path
from tqdm import tqdm
from PIL import Image

# å¯¼å…¥ä»“åº“ç»„ä»¶
from dataset import ImageCaptionDataset, collate_fn
from train_newbie_lora import load_model_and_tokenizer, setup_lora
from peft import set_peft_model_state_dict
from safetensors.torch import load_file
from transport import create_transport

def parse_args():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ SDPO ç¦»çº¿åå¥½æ•°æ®é›†æ ·æœ¬æ± ")
    parser.add_argument("--config_file", type=str, required=True, help="è®­ç»ƒæ—¶ä½¿ç”¨çš„ .toml é…ç½®æ–‡ä»¶ (lora.toml)")
    parser.add_argument("--lora_path", type=str, required=True, help="SFT è®­ç»ƒäº§å‡ºçš„ LoRA æƒé‡è·¯å¾„ (PEFT æ–‡ä»¶å¤¹ç›®å½•)")
    parser.add_argument("--output_dir", type=str, default="./sdpo_data_v1", help="æ ·æœ¬å›¾ç‰‡å­˜æ”¾ç›®å½•")
    parser.add_argument("--num_samples", type=int, default=4, help="æ¯ä¸ª Prompt ç”Ÿæˆçš„æ ·æœ¬æ•°é‡ (N)")
    parser.add_argument("--steps", type=int, default=28, help="ç”Ÿæˆæ­¥æ•°")
    parser.add_argument("--cfg_scale", type=float, default=5, help="Classifier-Free Guidance å¼ºåº¦")
    parser.add_argument("--device", type=str, default="cuda", help="ä½¿ç”¨è®¾å¤‡")
    return parser.parse_args()

@torch.no_grad()
def main():
    args = parse_args()
    
    # 1. åŠ è½½é…ç½®
    with open(args.config_file, 'r', encoding='utf-8') as f:
        config = toml.load(f)
    
    # å…³é—­æ•°æ®å¢å¼ºéƒ¨åˆ†ä»¥ä¿è¯æ•°æ®å¯¹åº”å…³ç³»
    config['Model']['shuffle_caption'] = False
    config['Model']['caption_dropout_rate'] = 0.0
    config['Model']['caption_tag_dropout_rate'] = 0.0
    config['Model']['drop_artist_rate'] = 0.0
    
    os.makedirs(args.output_dir, exist_ok=True)
    gen_images_dir = os.path.join(args.output_dir, "generated")
    os.makedirs(gen_images_dir, exist_ok=True)

    # 2. åŠ è½½æ¨¡å‹ä¸ LoRA
    # æ³¨æ„ï¼šç”Ÿæˆéœ€è¦å®Œæ•´çš„ VAE å’Œ Text Encoder
    model, vae, text_encoder, tokenizer, clip_model, clip_tokenizer = load_model_and_tokenizer(config)
    
    # åº”ç”¨ LoRA
    model = setup_lora(model, config)
    if os.path.isdir(args.lora_path):
        # å¦‚æœæ˜¯ PEFT ç›®å½•
        from peft import PeftModel
        model = PeftModel.from_pretrained(model, args.lora_path)
    else:
        print(f'{args.lora_path} is not a valid PEFT LoRA directory pathï¼')
    
    model.to(args.device).eval()
    vae.to(args.device).eval()
    text_encoder.to(args.device).eval()
    clip_model.to(args.device).eval()

    # 3. åˆå§‹åŒ–æ•°æ®é›† (è·å–åŠ¨æ€åˆ†ç®±ç»“æœ)
    # ä¸å¯ç”¨ cache ä»¥ä¾¿ç›´æ¥è¯»å–åŸå§‹è·¯å¾„ä¿¡æ¯
    dataset = ImageCaptionDataset(
        train_data_dir=config['Model']['train_data_dir'],
        resolution=config['Model']['resolution'],
        enable_bucket=config['Model'].get('enable_bucket', True),
        use_cache=False,
        min_bucket_reso=config['Model'].get('min_bucket_reso', 256),
        max_bucket_reso=config['Model'].get('max_bucket_reso', 2048),
        bucket_reso_step=config['Model'].get('bucket_reso_step', 64),
        enable_wildcard=False, # æ­¤å¤„å…ˆå…³é—­ï¼Œæˆ‘ä»¬åœ¨å¾ªç¯ä¸­æ‰‹åŠ¨å¤„ç† split
    )

    # 4. é¢„è®¡ç®—æ— æ¡ä»¶ç‰¹å¾ (ç”¨äº CFG)
    print("é¢„è®¡ç®—è´Ÿå‘ (Unconditional) ç‰¹å¾...")
    # Gemma æ— æ¡ä»¶
    uncond_input = tokenizer([""], padding=True, return_tensors="pt").to(args.device)
    uncond_outputs = text_encoder(**uncond_input, output_hidden_states=True)
    uncond_cap_feats = uncond_outputs.hidden_states[-2]
    uncond_cap_mask = uncond_input.attention_mask
    # CLIP æ— æ¡ä»¶
    uncond_clip_input = clip_tokenizer([""], padding=True, return_tensors="pt").to(args.device)
    uncond_clip_text_pooled = clip_model.get_text_features(**uncond_clip_input)

    # 5. å¼€å§‹ç”Ÿæˆå¾ªç¯
    results = []
    gemma3_prompt = config['Model'].get('gemma3_prompt', "")
    scaling_factor = getattr(vae.config, 'scaling_factor', 0.3611)
    shift_factor = getattr(vae.config, 'shift_factor', 0.1159)

    print(f"å¼€å§‹ä¸º {len(dataset)} ä¸ªåŸå§‹æ ·æœ¬ç”Ÿæˆåå¥½å¯¹æ± ...")
    
    for i in tqdm(range(len(dataset))):
        img_path = dataset.image_paths[i]
        raw_caption = dataset.captions[i]
        # è·å–è¯¥å›¾ç‰‡åœ¨åˆ†ç®±ç­–ç•¥ä¸‹çš„ç›®æ ‡åˆ†è¾¨ç‡
        target_width, target_height = dataset.image_to_bucket[i]
        
        # å¤„ç† Wildcard: è·å–æ‰€æœ‰å¯èƒ½çš„ caption å˜ä½“
        captions_to_process = [raw_caption]
        if "<split>" in raw_caption:
            captions_to_process = [c.strip() for c in raw_caption.split("<split>") if c.strip()]

        for cap_idx, caption in enumerate(captions_to_process):
            # å‡†å¤‡ç¼–ç ç‰¹å¾
            with torch.no_grad():
                # ç¼–ç æ­£å‘ç‰¹å¾
                gemma_text = gemma3_prompt + caption
                pos_input = tokenizer([gemma_text], padding=True, return_tensors="pt").to(args.device)
                pos_outputs = text_encoder(**pos_input, output_hidden_states=True)
                pos_cap_feats = pos_outputs.hidden_states[-2]
                pos_cap_mask = pos_input.attention_mask
            
                pos_clip_input = clip_tokenizer([caption], padding=True, return_tensors="pt").to(args.device)
                pos_clip_text_pooled = clip_model.get_text_features(**pos_clip_input)

            # ç”Ÿæˆ N ä¸ªæ ·æœ¬
            gen_paths = []
            for n in range(args.num_samples):
                # åˆå§‹åŒ–å™ªå£° [B=1, C=16, H//8, W//8]
                latents = torch.randn(1, 16, target_height // 8, target_width // 8).to(args.device)
                
                # Rectified Flow Euler é‡‡æ ·æ­¥
                dt = 1.0 / args.steps
                for step in range(args.steps):
                    t_val = step / args.steps
                    t = torch.ones(1).to(args.device) * t_val
                    
                    # æ‹¼æ¥ Batch åŠ é€Ÿå‰å‘ (æ— æ¡ä»¶ + æœ‰æ¡ä»¶)
                    batched_latents = torch.cat([latents, latents], dim=0)
                    batched_t = torch.cat([t, t], dim=0)
                    batched_cap_feats = torch.cat([uncond_cap_feats, pos_cap_feats], dim=0)
                    batched_cap_mask = torch.cat([uncond_cap_mask, pos_cap_mask], dim=0)
                    batched_clip_pooled = torch.cat([uncond_clip_text_pooled, pos_clip_text_pooled], dim=0)
                    
                    # å‰å‘æ¨ç†
                    v_out = model(
                        batched_latents, 
                        batched_t, 
                        cap_feats=batched_cap_feats, 
                        cap_mask=batched_cap_mask, 
                        clip_text_pooled=batched_clip_pooled
                    )
                    
                    # æ‹†åˆ†é¢„æµ‹é€Ÿåº¦ v
                    v_uncond, v_cond = v_out.chunk(2)
                    
                    # CFG æ··åˆ
                    v_final = v_uncond + args.cfg_scale * (v_cond - v_uncond)
                    
                    # Euler æ›´æ–°
                    latents = latents + v_final * dt
                
                # VAE è§£ç 
                latents = (latents / scaling_factor) + shift_factor
                image = vae.decode(latents.to(vae.dtype)).sample
                image = (image / 2 + 0.5).clamp(0, 1)
                image = image.cpu().permute(0, 2, 3, 1).float().numpy()[0]
                image = Image.fromarray((image * 255).astype("uint8"))
                
                # ä¿å­˜å›¾ç‰‡
                safe_name = Path(img_path).stem
                gen_filename = f"{safe_name}_cap{cap_idx}_sample{n}.jpg"
                gen_path = os.path.join(gen_images_dir, gen_filename)
                image.save(gen_path, quality=100)
                gen_paths.append(os.path.abspath(gen_path))

            # è®°å½•æ•°æ®ç»“æ„
            results.append({
                "caption": caption,
                "real_image_path": os.path.abspath(img_path),
                "generated_image_paths": gen_paths,
                "resolution": [target_width, target_height],
                "meta": {
                    "lora_source": args.lora_path,
                    "steps": args.steps,
                    "cfg_scale": args.cfg_scale,
                    "sampler": "euler"
                }
            })

    # 6. è¾“å‡º JSON æ•°æ®åº“
    output_json = os.path.join(args.output_dir, "dataset.json")
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ‰ ç¦»çº¿æ•°æ®åº“å·²å»ºç«‹: {output_json}")
    print(f"æ€»è®¡æ ·æœ¬æ•°: {len(results)}")

if __name__ == "__main__":
    main()
